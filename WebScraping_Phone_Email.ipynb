{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping startup websites from the meity startuphub page and save them in file.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoscraper import AutoScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://meitystartuphub.in/startups/startup-wall?search=&page=1&domain=&location=&stage=&type=\n",
      "['www.cubical.in', 'www.signzy.com', 'www.purpledocs.com', 'www.saylussmedicare.com', 'www.rnt.ai', 'www.smartjoules.co.in/', 'www.ireeworld.com', 'www.indusos.com', 'wimerasys.com', 'www.invilogic.com/', 'agrevolution.in', 'www.ipsindia.co.in', 'www.picturetime.in', 'www.sequretek.com', 'netobjex.com', 'www.wijungle.com', 'gcrs.co.in/']\n",
      "https://meitystartuphub.in/startups/startup-wall?search=&page=2&domain=&location=&stage=&type=\n",
      "['www.wemakescholars.com/', 'lecturenotes.in', 'www.flatpebble.com', 'fingpay.co.in', 'www.scholr.com', 'www.safalstartup.com', 'www.ithinklogistics.com', 'www.log9materials.com/', 'www.learnpick.in', 'www.wisig.com', 'www.volumetree.com', 'www.calligotech.com/', 'www.alcodex.com', 'pureev.in', 'www.examly.io', 'www.mpoverindia.com', 'www.trakitnow.com', 'phoenixrobotix.com', 'www.anytimeloan.in']\n",
      "https://meitystartuphub.in/startups/startup-wall?search=&page=3&domain=&location=&stage=&type=\n",
      "['effiasoft.com/', 'soulpageit.com/', 'www.chipspirit.com', 'www.legalkart.com', 'www.carenx.com', 'www.acculytixs.com', 'www.novaglobal.in', 'satsure.co', 'www.anyemi.com', 'cenacle.co.in/', 'lab-to-market.com/contact/', 'workruit.com', 'www.optimizedelectrotech.com', 'www.anvationlabs.com', 'www.fastechfashions.com', 'www.scarlettmoose.com/', 'www.5cnetwork.com', 'www.superprocure.com']\n",
      "https://meitystartuphub.in/startups/startup-wall?search=&page=4&domain=&location=&stage=&type=\n",
      "['www.synergyteletech.com', 'www.sensegiz.com', 'www.automovill.com/', 'www.mobismartcard.com', 'aadiwasijanjagruti.org', 'roushanseva.in', 'www.indiamart.com/printskool/', 'isthriwala.com', 'saarthi.ai', 'www.grinntech.com', 'www.lithionpower.com', 'www.aviconn.in', 'www.emc2.com', 'skillangels.com', 'www.kriscoagrotech.com', 'www.telebeacon.com', 'codeglobal.in/']\n",
      "https://meitystartuphub.in/startups/startup-wall?search=&page=5&domain=&location=&stage=&type=\n",
      "['wesecureapp.com', 'adstringo.in', 'www.ineshenergy.com', 'Telemo.io', 'f6designandtrainingsolution.com/', 'hrbot.co', 'www.estatemanservice.in', 'codebuckets.in', 'www.innotechaqua.com/', 'pharoslife.in', 'www.vizaratech.com', 'www.roadexpress.in', 'www.helpguru.com', 'www.idreameducation.org/', 'ziroh.com', 'nexrea.com', 'gofloaters.com', 'www.phoenixanalytics.in', 'www.numocity.com']\n"
     ]
    }
   ],
   "source": [
    "wanted_list = ['www.cubical.in', 'www.wemakescholars.com/', 'effiasoft.com/', 'www.synergyteletech.com', 'wesecureapp.com']\n",
    "# wanted_list = ['www.cubical.in']\n",
    "\n",
    "scraper = AutoScraper()\n",
    "for i in range(1, 6):\n",
    "    url = f\"https://meitystartuphub.in/startups/startup-wall?search=&page={i}&domain=&location=&stage=&type=\"\n",
    "    print(url)\n",
    "    result = scraper.build(url, wanted_list)\n",
    "    with open(\"file.txt\", 'a') as file:\n",
    "        for row in result:\n",
    "            s = \"\".join(map(str, row))\n",
    "            file.write(s+'\\n')\n",
    "    file.close()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Contact Details from the desired organization page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type y/n whether you want to enter the organization name: y\n",
      "enter the organization: purpledocs\n",
      "Type y/n whether you want to enter the organization name: y\n",
      "enter the organization: rnt.ai\n",
      "Type y/n whether you want to enter the organization name: smartjoules\n",
      "Type y/n whether you want to enter the organization name: y\n",
      "enter the organization: smartjoules\n",
      "Type y/n whether you want to enter the organization name: y\n",
      "enter the organization: ireeworld\n",
      "Type y/n whether you want to enter the organization name: y\n",
      "enter the organization: indusos.\n",
      "Type y/n whether you want to enter the organization name: y\n",
      "enter the organization: ipsindia\n",
      "Type y/n whether you want to enter the organization name: n\n",
      "Thankyou...\n",
      "searched home url:  https://www.purpledocs.com/\n",
      "searched contact url: https://www.purpledocs.com/contact-us/\n",
      "\n",
      " {\n",
      "  \"Searches\": \"purpledocs\",\n",
      "  \"website\": \"https://www.purpledocs.com/\",\n",
      "  \"Email\": [],\n",
      "  \"Phone\": [\n",
      "    \"099099 0353\",\n",
      "    \"081786 8839\",\n",
      "    \"097267 9756\",\n",
      "    \"99099 0353\",\n",
      "    \"81786 8839\",\n",
      "    \"97267 9756\"\n",
      "  ]\n",
      "}\n",
      "searched home url:  https://www.rabbitandtortoise.com/\n",
      "\n",
      " {\n",
      "  \"Searches\": \"rnt.ai\",\n",
      "  \"website\": \"https://www.rabbitandtortoise.com/\",\n",
      "  \"Email\": [],\n",
      "  \"Phone\": []\n",
      "}\n",
      "searched home url:  https://www.smartjoules.co.in/\n",
      "searched contact url: https://www.smartjoules.co.in/#contact\n",
      "\n",
      " {\n",
      "  \"Searches\": \"smartjoules\",\n",
      "  \"website\": \"https://www.smartjoules.co.in/\",\n",
      "  \"Email\": [\n",
      "    \"info@smartjoules.in\"\n",
      "  ],\n",
      "  \"Phone\": [\n",
      "    \" 2015-2021\"\n",
      "  ]\n",
      "}\n",
      "searched home url:  https://www.ireeworld.com/\n",
      "\n",
      " {\n",
      "  \"Searches\": \"ireeworld\",\n",
      "  \"website\": \"https://www.ireeworld.com/\",\n",
      "  \"Email\": [],\n",
      "  \"Phone\": []\n",
      "}\n",
      "searched home url:  https://www.indusos.com/\n",
      "searched contact url: https://www.indusos.com/contact-us/\n",
      "\n",
      " {\n",
      "  \"Searches\": \"indusos.\",\n",
      "  \"website\": \"https://www.indusos.com/\",\n",
      "  \"Email\": [\n",
      "    \"grievances@indusos.com\",\n",
      "    \"info@indusos.com\"\n",
      "  ],\n",
      "  \"Phone\": [\n",
      "    \"9122 4510\",\n",
      "    \" 2020-2021\",\n",
      "    \"022 4510\"\n",
      "  ]\n",
      "}\n",
      "searched home url:  http://www.ipsindia.co.in/\n",
      "searched contact url: http://www.ipsindia.co.in/#contact-us\n",
      "\n",
      " {\n",
      "  \"Searches\": \"ipsindia\",\n",
      "  \"website\": \"http://www.ipsindia.co.in/\",\n",
      "  \"Email\": [\n",
      "    \"contact@ipsindia.co.in\"\n",
      "  ],\n",
      "  \"Phone\": [\n",
      "    \" 022 6282\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "\n",
    "from tld import get_tld\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "try: \n",
    "\tfrom googlesearch import search \n",
    "except ImportError: \n",
    "\tprint(\"No module named 'google' found\")\n",
    "\t\n",
    "#code to googlesearch\n",
    "search_list = []\n",
    "# open the web_urls file to clean the already present data   \n",
    "f = open(\"web_urls.txt\", \"r+\")  \n",
    "  \n",
    "# absolute file positioning \n",
    "f.seek(0)  \n",
    "  \n",
    "# to erase all data  \n",
    "f.truncate()\n",
    "\n",
    "k = 0\n",
    "m=True\n",
    "while(m):\n",
    "        s=input('Type y/n whether you want to enter the organization name: ')\n",
    "        if(s=='y'):\n",
    "                query = input('enter the organization: ')\n",
    "                search_list.append(query)\n",
    "                file = open(\"web_urls.txt\", \"a\")\n",
    "                for o in search(query, tld=\"co.in\", num=1, stop=1, pause=2): \n",
    "                        url_site=o\n",
    "                        file.write(url_site)\n",
    "                        file.write('\\n')\n",
    "        if(s=='n'):\n",
    "                m=False \n",
    "                print('Thankyou...')\n",
    "                file.close()\n",
    "\n",
    "# function to remove duplicates\n",
    "def remove_dup_email(x):\n",
    "  return list(dict.fromkeys(x))\n",
    "\n",
    "def remove_dup_phone(x):\n",
    "  return list(dict.fromkeys(x))\n",
    "\n",
    "def get_email(html):\n",
    "    try:\n",
    "        email = re.findall(\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,3}\",html)\n",
    "        nodup_email = remove_dup_email(email)\n",
    "        return nodup_email\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_phone(html):\n",
    "    try:\n",
    "        phone = re.findall(r\"(\\d{2} \\d{3,4} \\d{3,4})\", html)\n",
    "        phone1= re.findall(r\"((?:\\d{2,3}|\\(\\d{2,3}\\))?(?:\\s|-|\\.)?\\d{3,4}(?:\\s|-|\\.)\\d{4})\",html)\n",
    "        for p in phone1:\n",
    "             phone.append(p)\n",
    "        nodup_phone = remove_dup_phone(phone)\n",
    "        return nodup_phone\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "urls = ''\n",
    "\n",
    "\n",
    "# load website url links\n",
    "with open('web_urls.txt', 'r') as f:\n",
    "    for line in f.read():\n",
    "        urls +=line\n",
    "\n",
    "#convert a string to a list of urls\n",
    "urls = list(filter(None, urls.split('\\n')))\n",
    "\n",
    "# Looping over the urls\n",
    "for url in urls:\n",
    "\n",
    "    #http requests to the urls\n",
    "    res = requests.get(url)\n",
    "    print('searched home url: ', res.url) \n",
    "\n",
    "    # parse the response\n",
    "    info = BeautifulSoup(res.text,'lxml')\n",
    "\n",
    "\n",
    "    # extract contact data from home url\n",
    "    emails_home = get_email(info.get_text())\n",
    "    phones_home = get_phone(info.get_text())\n",
    "\n",
    "    emails_f = emails_home\n",
    "    phones_f = phones_home\n",
    "\n",
    "    \n",
    "    # create a data structure to store the contacts\n",
    "    contacts_f = {'Searches':search_list[k],'website':res.url,'Email':'','Phone':''}\n",
    "    k=k+1\n",
    "\n",
    "    # extract contact of the link if available\n",
    "    try:\n",
    "        contact = info.find('a', text = re.compile('contact', re.IGNORECASE))['href']\n",
    "        if 'http' in contact:\n",
    "            contact_url = contact\n",
    "        else:\n",
    "            contact_url = res.url[0:-1] + contact\n",
    "\n",
    "        # searching contact URL\n",
    "        res_contact = requests.get(contact_url)\n",
    "\n",
    "        contact_info = BeautifulSoup(res_contact.text, 'lxml').get_text()\n",
    "\n",
    "\n",
    "        print('searched contact url:', res_contact.url)\n",
    "\n",
    "        # extract contact data\n",
    "        emails_contact = get_email(contact_info)\n",
    "        phones_contact = get_phone(contact_info)\n",
    "\n",
    "        #combining email contacts and email home into a single list\n",
    "\n",
    "        emails_f = emails_home\n",
    "\n",
    "        for ele1 in emails_contact:\n",
    "            emails_f.append(ele1)\n",
    "\n",
    "        #combining phone contacts and phone contacts into a single list\n",
    "    \n",
    "        phones_f = phones_home\n",
    "\n",
    "        for ele2 in phones_contact:\n",
    "            phones_f.append(ele2)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # removing duplicates\n",
    "    emails_f = remove_dup_email(emails_f)\n",
    "    phones_f = remove_dup_email(phones_f)\n",
    "\n",
    "    contacts_f['Email']= emails_f\n",
    "    contacts_f['Phone']= phones_f\n",
    "    \n",
    "    # converting into a data set\n",
    "    print('\\n', json.dumps(contacts_f, indent=2))\n",
    "\n",
    "    # dumping the data into the csv file\n",
    "    with open('organization_info.csv', 'a') as f:\n",
    "        #creater csv writer object\n",
    "        writer = csv.DictWriter(f, fieldnames=contacts_f.keys())\n",
    "        #writer.writeheader()\n",
    "        \n",
    "        #append rows to the csv\n",
    "        writer.writerow(contacts_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
